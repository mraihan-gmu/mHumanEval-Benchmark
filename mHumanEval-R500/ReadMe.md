# **mHumanEval-R500 Overview** üé≤

**mHumanEval-R500** is a diverse subset designed for evaluating the robustness of a Code LLM across various languages and prompt qualities.

## Key Features ‚ú®

- **Total Prompts**: 500 
- **Languages**: 204 
- **Prompts per Language**: At least 2

| **Feature**               | **Description**                                                         |
|---------------------------|-------------------------------------------------------------------------|
| **Total Prompts**         | 500                                                                     |
| **Languages**             | 204                                                                     |
| **Prompts per NL**        | At least 2                                                              |

## Purpose üéØ

**mHumanEval-R500** provides a varied set of prompts randomly selected from the extensive mHumanEval dataset, making it ideal for testing the generalization and robustness of Code LLMs:

- **Random Selection**: Prompts are randomly chosen from a pool of 33,456, ensuring a mix of topics and difficulty levels.
- **Multilingual Coverage**: Ensures at least 2 prompts from each of the 204 natural languages (NLs).

## Comparison with mHumanEval-mini üîç

While **mHumanEval-mini** focuses on high-quality, single prompts from each language, **mHumanEval-R500** offers:

- **More Prompts**: 500 prompts for a more comprehensive evaluation.
- **Varied Quality**: A mix of prompt qualities, from best to average, providing a broader testing ground.

