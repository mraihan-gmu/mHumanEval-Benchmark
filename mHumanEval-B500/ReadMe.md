# **mHumanEval-B500 Overview** ðŸ“‰

**mHumanEval-B500** comprises the subset of prompts that scored the lowest in quality evaluations from the comprehensive mHumanEval dataset.

## Key Features âœ¨

- **Total Prompts**: 500
- **Selection Criteria**: Lowest quality based on BERTScore from round-trip translations

| **Feature**               | **Description**                                              |
|---------------------------|--------------------------------------------------------------|
| **Total Prompts**         | 500                                                          |
| **Selection Criteria**    | Lowest evaluation scores using BERTScore                     |

## Purpose ðŸŽ¯

**mHumanEval-B500** is intended for testing the robustness and error-handling capabilities of Code LLMs when dealing with lower quality prompts. This subset provides insight into how models perform under less-than-ideal conditions:

- **Low-Quality Prompts**: Selected from a pool of 33,456 prompts, these represent the lowest scoring prompts in terms of clarity, coherence, and informativeness.

